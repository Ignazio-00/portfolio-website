# https://www.robotstxt.org/robotstxt.html
User-agent: *
Allow: /

# Sitemap
Sitemap: https://ignaziobalistreri.dev/sitemap.xml

# Crawl-delay (optional - helps with server load)
Crawl-delay: 1

# Block specific files/directories if needed
# Disallow: /private/
# Disallow: *.pdf$

# Allow all common crawlers
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /
